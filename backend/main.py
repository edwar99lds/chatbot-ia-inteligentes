# backend/main.py
from fastapi import FastAPI
from pydantic import BaseModel
import time  # â± Para medir latencia
from utils.logger import log_interaction  # ðŸ“Š Importa el registrador de mÃ©tricas
from dotenv import load_dotenv
import os

# --- Cargar variables de entorno ---
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_PATH = os.getenv("DB_PATH", "data/vectorstore")
DEBUG = os.getenv("DEBUG", "False").lower() == "true"

if DEBUG:
    print(f"ðŸ” OPENAI_API_KEY: {OPENAI_API_KEY[:5]}******")
    print(f"ðŸ—‚ï¸ DB_PATH: {DB_PATH}")
    print(f"ðŸž DEBUG mode: {DEBUG}")

app = FastAPI(
    title="ChatBot IA - Backend",
    description="Backend para ChatBot acadÃ©mico sobre IA",
    version="1.0"
)

# Modelo de datos para la peticiÃ³n
class QueryRequest(BaseModel):
    question: str
    mode: str  # "breve" o "extendido"


# Ruta principal (saludo)
@app.get("/")
def read_root():
    return {"message": "Bienvenido al backend del ChatBot de IA - Universidad de Caldas"}


# Ruta de consulta
@app.post("/query")
def get_answer(request: QueryRequest):
    """
    Responde con un texto simulado y un par de citas de ejemplo.
    Luego aquÃ­ se integrarÃ¡ el pipeline RAG real.
    TambiÃ©n registra mÃ©tricas de uso (anonimizadas).
    """
    start_time = time.time()  # Inicia el contador de latencia

    # --- SimulaciÃ³n de respuesta (dummy) ---
    answer_text = (
        f"Hola ðŸ‘‹, soy el backend del ChatBot IA. "
        f"Tu pregunta fue: '{request.question}'. "
        f"Estoy en modo '{request.mode}'."
    )

    citations = [
        "UNESCO - Informe de Ã‰tica en IA 2023",
        "AI Act - RegulaciÃ³n Europea de IA 2024"
    ]

    # --- CÃ¡lculo de mÃ©tricas ---
    latency = time.time() - start_time  # Latencia en segundos
    cost = 0.0003  # ðŸ’° costo simbÃ³lico (simulaciÃ³n de uso de modelo)
    model_name = "mock-model-v1"

    # --- Registro en logs ---
    log_interaction(
        question=request.question,
        model=model_name,
        latency=latency,
        cost=cost
    )

    # --- Respuesta ---
    return {
        "answer": answer_text,
        "citations": citations
    }

# Backend - Terminal de visual
# python -m uvicorn backend.main:app --reload --port 8000


# Backend - Terminal de visual
# python -m uvicorn backend.main:app --reload --port 8000

#Frontend (lo abrÃ­ en git bash here)
# source venv/Scripts/activate
# streamlit run app_streamlit.py

# Ejecutar el programa:
# docker compose up

# Parar el programa
# docker compose down

# si se cambian dependencias:
# docker-compose up --build

# cloudflared tunnel --url http://localhost:8501
# cloudflared tunnel --url http://localhost:8501 --name chatbot-edwar

# python evaluate.py


# pip freeze > requirements.txt
